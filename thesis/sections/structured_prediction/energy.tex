A basis of both inference and parameter training is a proper formulation of an energy function, which reflects a probability of occurrence of a given configuration of variables in a system. In semantic image segmentation, without correctly defined energy function it will not be possible to differentiate objects of different classes. As it has already been presented, energy is composed of two terms: unary potential and pairwise potential. 
\begin{equation}
    E(y,x) = \sum_{i \in V}{E_1(y_i,x_i)} +  \sum_{i,j \in V}{E_2(y_i,y_j)})
\end{equation}
Both of those terms are needed for Conditional Random Fields to be successful. 

\subsection{Unary potential}
In semantic image segmentation, the unary component of the energy function predicts a label of a given pixel or region, based on some observed features of this part of an image. Though it can be modelled with a number of different methods it is always responsible for assigning the same label to input nodes that are characterised by similar features. A feature can be described as a numeric representation of row data that is fetched from an image. The process of extracting and selecting those representations is a challenging task as without properly chosen features that provide all the required information to the model, it would not be possible to perform the assigned task. What is more, it usually accounts for the vast majority of the time that is needed to perform a machine learning task \cite{features_oreilly}. Proper formulation of a feature vector is a crucial step in machine learning as it reflects not only the level of the model complexity but also its performance. However, the relevance of available features is strictly bound to data and to the chosen model and due to to the large diversity in both data and models it is extremely difficult to generalise the process of feature engineering. Depending on the task some models can perform well given a large number of features while others require less, but more informative features. Though there exist some automatic or semiautomatic tools for feature engineering \cite{python_h2o, python_tpot, python_auto-sklearn}, that aim to make feature extraction less problem specific, still the vast majority of machine learning tasks use the traditional approach of manual feature selection \cite{feature_engineering}. 
Features used for any task within the area of image processing can be either low- or high-level \cite{feature_extraction_book}. Low-level features are such features that can be automatically extracted from an image such as pixel intensity, gradients, colours, edges or textures. They do not give any information about spatial relations between regions of an image. On the other hand, high-level features correlate data obtained from low-level features with a content of an image to provide information about shapes and objects, thus having a semantic meaning. Feature engineering is a time-consuming and demanding task of its own and it is not a part of this dissertation. Therefore, to stay within the scope of the thesis, feature extraction is limited only to features that are based solely on colour-related data.

In the most basic form unary potential of a given factor between an input node $x_i$ and an output node $y_i$ is specified by a linear relation between feature function $\varphi(x)$ and learned parameters $w(y)$ \cite{Nowozin} as in equation \ref{eq:unary_potential_basic}.
\begin{equation}
    \label{eq:unary_potential_basic}
    E_1(y_i,x_i,w) =  \left \langle w(y_i), \varphi(x_i) \right \rangle 
\end{equation}
In the most basic form, feature function outputs a feature vector $\phi$ that is composed of colour values of each individual pixel. A colour value can be expressed in a numerical form with the use of a chosen colour space. A colour space is a mathematical model representing a colour value as a tuple of typically three or four numbers. The most popular colour space being based on the physiology of a human eye is RGB model, in which an arbitrary colour value is denoted as a combination of values of three primary colours, which are red, green, and blue. A different colour space, named CIELAB, was created in other to mimic a human perception of colours. The model is constructed on three axes, first being axis L* representing Lightness with values between 0 and 100. Axes a* and b* encode green-red and blue-yellow components respectively. There are many other colour spaces such as CMYK, HSV, HSL or YUV, each of them being more suitable for certain tasks \cite{colour_space}. Nevertheless, the choice of colour space in which colour features will be encoded in the prediction model can influence its performance. For simplicity, in this chapter RGB colour space will be assumed. Hence, a feature vector $\phi$ for a given pixel is composed of three features, each representing a different component of the chosen colour space as in equation \ref{eq:fi_RGB}.
\begin{equation}
    \label{eq:fi_RGB}
    \phi(x_i) = \begin{bmatrix}
        \varphi_R\\ 
        \varphi_G\\ 
        \varphi_B
    \end{bmatrix}
\end{equation}

When it comes to parameters of the unary potential they are expressed in terms of weights that are learned in the training process. With every label $y$ there is a separate weight vector $w$ associated, as in as in equation \ref{eq:weight_RGB}, and to compute unary potential a concatenation of weight vectors for each label is needed.
\begin{equation}
    \label{eq:weight_RGB}
     w(y) = \begin{bmatrix}
        w_R\\ 
        w_G\\ 
        w_B
    \end{bmatrix}
\end{equation}
This distinction is required as each object class can be best characterised by different features. For example, given an object with label \textit{grass} a green component of the feature vector should be prioritised, while for label \textit{sky} it should be a blue component. As the task of image segmentation, meaning finding an optimal prediction $y^*$ for each pixel in an image, is equivalent to the problem of energy minimisation, weights which are associated with prioritised features should have relatively small values. Hence, with the same example of two labels \textit{grass} and \textit{sky} proper configuration of weights would be as in equation \ref{eq:weights_labels}.
\begin{equation}
    \label{eq:weights_labels}
    \begin{matrix} 
        w_{grass} = \begin{bmatrix}
            1.0\\ 
            0.0\\ 
            1.0
            \end{bmatrix}  
        & & &
        w_{sky} = \begin{bmatrix}
            1.0\\ 
            1.0\\ 
            0.0
            \end{bmatrix} 
    \end{matrix}
\end{equation}
Then, for example for fully blue pixels, with feature vector $\phi = [0.0, 0.0, 1.0]$, the unary component of energy will be equal to 0.0 for label \textit{sky} and 1.0 for label \textit{grass}. Thus, label \textit{sky} will be chosen for such pixels, as the energy for this label is smaller than for other labels. 
Hence. to obtain the unary component of the energy function for a given pixel $x_i$ with a given label $y_i$ from a set of labels $L$, a corresponding weight vector needs to be multiplied by the feature vector as in equation \ref{eq:e1_weight_vector}. 
\begin{equation}
    \label{eq:e1_weight_vector}
    E_1(y_i,x_i,w)= 
    \begin{cases}
        \left \langle w_{1,0}, \phi({x_i}) \right \rangle , &  \text{if } y_i = 0\\ 
        \left \langle w_{1,1}, \phi({x_i}) \right \rangle , & \text{if } y_i = 1\\
         ...& ...\\ 
        \left \langle w_{1,L}, \phi({x_i}) \right \rangle , & \text{if } y_i = L\\  
    \end{cases}
\end{equation}


Presented formulation of the unary potential is the most trivial one, however, it can also be applied also for more complex models and the only difference is in a way the feature vector is constructed. The goal of energy function is to provide a measure of how well pixels in an image are labelled and any feature vector that can be expressed in a numerical form is possible to be applied for the unary term of the energy. A feature function does not need to provide a feature vector that is composed of low-level features extracted from an image. Instead, a higher level of abstraction can be used to make features of a given pixel dependent on other classifiers. One example would be a Decision Tree Field approach in which factors that form unary potential are dependent on Decision Trees \cite{crf_decision_trees}. Another and a very popular and widely described approach to semantic image segmentation is to model a feature vector for unary potential by utilisation of outputs of a Convolutional Neural Network \cite{inference_crf, crf_cnn1, crf_cnn2}. However, incorporating an additional classifier into the unary potential of Conditional Random Fields is not the only possibility to improve its performance. Another way is to treat feature function as a probability density function representing a relation between labels and features. Hence, instead of operating on a feature vector that directly represents some properties of a given pixel, or on outputs that were assigned by an auxiliary classifier, such feature function is introduced that will be able to provide a conditional probability of assigning a label to the current pixel given its features. This probability is expressed in terms of log-likelihood as in equation \ref{unary_potential_generic}.
\begin{equation}
    \label{unary_potential_generic}
    \varphi(x_i,y_i) = -\log p(y_i|x,i)
\end{equation}
A given pixel $i$ is represented as a set of features, hence, computation of unary potential is equivalent to finding probability distributions of every label $y_i$ conditioned on a set of features $f_i$, which can be parametrised with additional parameter $\theta$. 
\begin{equation}
p(y_i | f_i, \theta)
\end{equation}
Conditional probabilities of $p(y_i | f_i)$ are not known, however, according to Bayes theorem, they can be found if there is a prior knowledge on a reverse probabilities $p(f_i | y_i)$ as in equation \ref{eq:bayes_theorem}.
\begin{equation}
    \label{eq:bayes_theorem}
    p(y_i | f_i) = \frac{p(f_i | y_i) \cdot p(y_i)}{p(f_i)}
\end{equation}
As probabilities of all possible labellings for a given pixel needs to sum to 1, a term $p(f_i)$ acts as a normalising constant in this equation. Therefore, it can be obtained by equation \ref{eq:bayes_theorem_feature_probability}.
\begin{equation}
    \label{eq:bayes_theorem_feature_probability}
    p(f_i) = \sum_{y_i \in \pazocal{Y}}{(p(f_i | y_i) \cdot p(y_i))} 
\end{equation}

Hence, to calculate a unary potential for a pixel in a given image it is necessary to know the probability of occurrence of the chosen label and a probability of pixel features conditioned on this label. Both of them can be found from a probability distribution that needs to be modelled in an additional training process. With a set of correctly labelled training examples a label probability $p(y_i)$ is straightforward to calculate as it enough to find the ratio between pixels with a given label assigned and all pixels in a training set. Computation of $p(f_i | y_i)$ can be fairly complex, as a dimension of this distribution is dependent on the number of chosen features. However, assuming that chosen features are independent of each other, a problem of finding N-dimensional probability distribution can be transformed into N problems of finding 1-dimensional distributions as in equation \ref{eq:distribution_dimension}.
\begin{equation}
    \label{eq:distribution_dimension}
    p(f_i|y_i) = \prod_{k=1}^{K}p(f_{i,k}|y_i)
\end{equation}

Any number and kind of features can be chosen as long as they can be expressed in a numeric way. The method of assessing a conditional probability between a feature and a label is dependent on a feature type. In general, features can be either discrete or continuous. The first type also known as categorical data, represent such features that can have only one value from a set of predefined values. Then, to get a probability $p(f_{i,k}|y_i)$ for a pixel from a given test image, it is enough to have probabilities of every possible value of the given feature computed beforehand. Once computed during the training process, such probabilities are applicable for any new test image, as for discrete features the whole input domain is covered. For instance, if an image will be first subjected to the process of colour quantisation in order to limit the number of available colours in an image for example by using a 32 colour palette, then a feature representing a colour of a pixel would be a discrete feature taking one out of 32 possible values. Hence, in order to get the probability distribution of such feature, it is enough to count how many times a given colour value appeared in the training set.

On the other hand, for continuous features the input domain is infinite, and therefore it is infeasible to compute a probability of occurrence of each value beforehand. A probability of continuous variables is represented by a probability density function. This function describes a relative likelihood that the variable of interest will have a given value. Then, the probability that the variable will fall within a certain range of values is equal to the integral of the variable density over this range \cite{statistics}. However, usually no information is available on the type of features distribution, nor on parameters that define it. Fortunately, given a training set of data samples, it is possible to estimate the probability density function of a continuous variable even without knowing its type. It can be achieved by kernel density estimation, also known as Parzen–Rosenblatt window method. The idea of this method is based on superposing kernel functions that are placed over observations and later scaling them in order to create one smooth function that is an approximation of all data points. Hence, to obtain probability $p(f_{i,k}|y_i)$  of a chosen feature $k$ of pixel $i$ there is a need to estimate probability distribution $p(f_{i,k})$ based on all $N$ pixels labelled with $y_i$ according to formula \ref{eq:parzen_estimation}.
\begin{equation}
    \label{eq:parzen_estimation}
    p(f_{i,k}) = \frac{1}{Nh_k} \sum_{n=1}^{N} \pazocal{K} (\frac{f_{i,k} - f_{i,k}^{n}}{h_k})
\end{equation}
Kernel, denoted as $\pazocal{K}$, is a probability density function with known distribution that is symmetric around 0. The choice of kernel determines the shape of those individual functions around data points which will be averaged. Gaussian function, which models normal distribution, is an example of a function that can be used as a kernel. It is presented in formula \ref{eq:gaussian_kernel}.
\begin{equation}
    \label{eq:gaussian_kernel}
    \pazocal{K}(u) = \frac{1}{\sqrt{2\pi}}e^{-\frac{1}{2}u^2}
\end{equation}
Other examples of kernel functions include uniform function, also known as rectangular window as in equation \ref{eq:uniform_kernel},
\begin{equation}
    \label{eq:uniform_kernel}
    \pazocal{K}(u) = \frac{1}{2}
\end{equation}
triangular function, presented in equation \ref{eq:triangular_kernel},
\begin{equation}
    \label{eq:triangular_kernel}
    \pazocal{K}(u) = 1 - \left \| u \right \|
\end{equation}
or parabolic function, named Epanechnikov kernel, which is shown in equation \ref{eq:parabolic_kernel}.
\begin{equation}
    \label{eq:parabolic_kernel}
    \pazocal{K}(u) = \frac{3}{4}(1-u^2)
\end{equation}
Bandwidth, denoted as $h$, is a hyperparameter of the kernel density estimation responsible for the level of data smoothing as it controls the width of individual superposed functions. Consequently, it reflects the range in which data points have a larger effect on an individual observation. The optimal choice of bandwidth is required for the good performance of the method. For large values of $h$ the function will be underfitted, meaning it will be too simple to model known observations, nor it will be able to predict probabilities of new data points. On the other hand, for values of $h$ that are too small, the model will exhibit overfitting. It will almost perfectly learn all training data, including noise, however, it will not be able to generalise for new data points. As any hyperparameter, bandwidth value should be subjected to the optimisation process before the estimation takes place. A commonly used method to find optimal values of hyperparameters is called cross-validation \cite{cross_validation}. In this method for every value of a hyperparameter that is under consideration, a set of observed data is divided into two partitions, a training set and a validation set. Then, the model is trained only with the first set and its performance is evaluated on the validation set. Typically, training and validation steps are performed multiple times using different partitions, so that most of the observed data will be validated against. Then, the results of each individual evaluation are aggregated to provide a measure of the model prediction performance. Optimal hyperparameter is the one for which this performance is the highest.

After finding the optimal value of the bandwidth $h$ and choosing the kernel function $\pazocal{K}$ kernel density estimation can be used to predict probability $p(f_{i,k})$ of a feature $k$ from pixel $i$ from an unknown test image. This requires iterating through all the training data in order to compute the difference between the value of this feature and the observed value that is needed for a kernel function. For large training sets, this becomes computationally complex. Moreover, this needs to be repeated for every feature of all pixels in a test image, which makes it even more time-consuming. Hence, for large training sets, other methods of probability estimation should be used. 

Parzen–Rosenblatt window method provides an accurate way to estimate the probability of an unknown sample based on the aggregated probability density function. However, this function is freshly generated each time estimation is needed. In order to limit the number of required calculations, there is a need to propose a method that will model the underlying probability density only once, during the training phase, and not every time probability of a new observation needs to be assessed. Unfortunately, without knowing parameters describing the generated probability distribution function there is no way to regenerate it without iterating through all data points. However, the problem can be simplified if the training data would be firstly binned. Binning is the process aimed to discretise continuous data by grouping them into intervals of equal size that are called bins. The simplest way to model binned data is to use histograms, which give information on the number of data samples that fell into a particular bin. Then, to obtain the probability of a feature $k$ of an unknown pixel $i$, it is enough to check to which bin its value falls and to calculate the ratio between the number of observations in a bin denoted as $b_{f_{i,k}}$ and all data points $N$ as in equation \ref{eq:histogram}.
\begin{equation}
    \label{eq:histogram}
    p(f_{i,k}) = \frac{b_{f_{i,k}}}{N}
\end{equation}
The number of observations in each bin can be computed only once, in a training phase, and it gives enough information to estimate the probability of a feature of any unknown pixel. The accuracy of this estimation is reflected by the number of bins used to create a histogram. If too few bins were used in a discretisation process, important data about observations, especially at the boundaries of bins, would be lost. On the other hand, the more bins are used, the more computationally complex the problem becomes. The number of bins is a hyperparameter of a histogram model, and as in the case of bandwidth in the Parzen–Rosenblatt window method, it should be found in an optimisation process.

After choosing an appropriate method of probability density estimation, it is possible to obtain a probability distribution of each feature $p(f_{i,k} | y_i)$ and consequently the whole distribution for a given image $p(f_i | y_i)$. This distribution is needed for Bayes' theorem, which was presented in formula \ref{eq:bayes_theorem}. Then, for every pixel from an unknown test image, it will be possible to get a probability of each label $p(y_i | f_i)$, and therefore calculate the unary potential $E_1(y_i,x_i,w)$.






\subsection{Pairwise potential}


On the other hand, pairwise potentials connect outputs of neighbouring nodes so that the situation in which input nodes that are in close proximity to each other will have the same label assigned. Again, in the simplest possible method, pairwise potentials are expressed in terms of two parameters which directly reflect energy. If two neighbouring nodes have the same label, then an energy of the factor between them will be equal to weight $w_1$, if not $w_2$.


When it comes to the second component of the energy function - a pairwise potential, it is responsible for smoothness of the predicted labels. It is aimed to handle situations in which there is some noise in an image as it penalises a situation in which two neighbouring pixels have a different label assigned. Thus, a sample pixel that is surrounded by pixels with some label will be more likely to have the same label even if the unary term is more prone to assign a different label to this pixel, because of its noised features. In the simplest form, a weight vector $\varphi$ is a two-dimensional vector with only two possible values $[1,0]$ for pairs with the same label, and $[0,1]$ for pairs with a different label, as presented in equation \ref{eq:fi_pairwise_two_labels}
\begin{equation}
    \label{eq:fi_pairwise_two_labels}
    \begin{matrix}
        \varphi_{y_i = y_j} = \begin{bmatrix}
            1 \\
            0
        \end{bmatrix} & & &
        \varphi_{y_i \neq y_j} = \begin{bmatrix}
            0 \\
            1
        \end{bmatrix}
    \end{matrix}
\end{equation}
Then, a weight vector is a two dimensional vector composed of weight $w_{2,0}$ which describes a situation in which neighbouring pixels have the same label, and weight  $w_{2,1}$ for a pair differing in assigned labels. 
\begin{equation}
    \label{eq:weight_pairwise}
    w(y_i, y_j) = \begin{bmatrix}
        w_{2,0} \\ 
        w_{2,1}
    \end{bmatrix}
\end{equation}
Similarly to the unary component of the energy function, the pairwise component can be described as a linear relation between weight and feature vectors. This is, in fact,  equivalent to expressing it in terms of two weights which directly reflect pairwise energy value. If two pixels that form neighbouring nodes in a factor graph have the same label assigned, then the energy of the factor between them will be equal to weight $w_{2,0}$, if they have a different label assigned, then it will be $w_{2,1}$ as in equation \ref{eq:energy_pairwise}. Again, as the state of smaller energy is the one that is going to be chosen for a given factor, if the situation of two neighbouring pixels having the same label is to be promoted, then $w_{2,0}$ should have a smaller value than $w_{2,1}$.
\begin{equation}
 \label{eq:energy_pairwise}
    E_2(y_i,y_j)=\begin{Bmatrix}
     w_1 & , y_i=y_j \\ 
     w_2 & , y_i \neq y_j
    \end{Bmatrix}
\end{equation}
The pairwise term of energy function provides higher-order information about relations neighbouring pixels, however, the notion of neighbours is different depending on the model used. In general, a neighbour is any pixel that is adjacent to the given pixel, however, neighbourhoods of larger size can also be used. In such a situation factor graphs are highly beneficial as they allow to model relations between even larger groups of pixels in a clear and straightforward way. What is more, pairwise potential can be utilised to take into account even more complex relations between pixels by introducing a measure on how different their classes are from each other. Then, the feature vector, instead of having only two possible values $0$ or $1$, can take any value within the range of $0$ and $1$ which reflects a measure of similarity between pixel classes. Then, the formula for a feature vector used to calculate a pairwise potential can be expressed as in the equation below.
\begin{equation}
    \label{eq:fi_pairwise}
    \varphi(y_i,y_j) = \begin{bmatrix}
        1 - \left | (y_i - y_j) \right | \\
        \left | (y_i - y_j) \right |
    \end{bmatrix}
\end{equation}
Assuming that the difference between labels $\left | (y_i - y_j) \right |$ is either 0 if labels are the same or 1 if labels are different, equation \ref{eq:fi_pairwise} is equivalent to equation \ref{eq:fi_pairwise_two_labels}.




NOTES :

****************************

features including contextual data

*****************************