Parameter training is a process of establishing weights of a model that are used for energy calculation while inferring the final result. For this task supervised learning is applied which uses a set of labelled train sample. Having them it is possible to construct a factor graph where not only observed variable nodes X are known but also labels for hidden nodes Y. Similarly to the task of inference, there are two main ways of parameter learning, first involving computation of marginal distributions named Maximum Likelihood Learning and the second requiring only MAP labelling \cite{markov_blake}. The latter method known as Max-Margin Learning has recently gained recognition as an alternative towards Max-Margin Learning, however, it is based on Support Vector Machines and not on Conditional Random Fields, which are the key topic of this dissertation. 

Maximum Likelihood Learning is a standard and most popular approach to parameter learning in CRFs. It is a generative method used to maximise conditional probability $P(X|Y,w^*)$ based on training objects with known X and Y and unknown weight vector w. Knowing that the state with the lowest energy of a factor graph is, in fact, the most probable state, the task of parameter learning can be expressed as a minimisation problem of a conditional probability, expressed in terms of energy over a set of all training samples denoted as $N$. To make sure that the model is not overfitted a regularisation term, with regularisation constant denoted as $\lambda$, can be added according to the formula \ref{eq:regularisation}.
\begin{equation}
    \label{eq:regularisation}
    w^* = \argmin_{w \in \mathbb{R}}{\lambda \left \| w \right \|^2} + \sum_{n=1}^{N}{E(x^n,y^n,w)} + \sum_{n=1}^{N}{\log{Z}(x^n,w)}
\end{equation}
Finding a minimum of a function is a typical task in machine learning and can be solved with various methods. The fundamental and most straightforward method of solving optimisation problems is named Gradient Descent \cite{optimisation_deep_learning}. It is an algorithm that updates parameters of an objective function in such a way to minimise an error between expected outputs of training examples and the actual output of the trained function. It is done by iteratively moving into a direction specified by a negative of a gradient of the objective. The algorithm starts by assigning arbitrary values to the parameters of the function and changing them slightly in each iteration, according to the calculated gradient. At each iteration t+1 parameters are updated by subtracting part of a value of the gradient. It is defined by a variable $\alpha$ being a step size hyperparameter, which can be either constant or changeable throughout a process of learning. Weight update is done according to the formula \ref{eq:weight_update}.
\begin{equation}
    \label{eq:weight_update}
    w_{(t+1)} = w(t) - \alpha_t \nabla (w_t)
\end{equation}
The process continues until the error is small enough to state that a local minimum, or global in case of convex functions, was reached. To apply the Gradient Descent method in an optimisation problem, an objective function has to be differentiable. Fortunately, a regularised conditional probability specified as a function of weight vector $w$, that is presented in formula \ref{eq:regularisation_prob}, is a smooth and convex function, hence Gradient Descent method can be used to find a global minimum of it.
\begin{equation}
    \label{eq:regularisation_prob}
    \pazocal{L} = \lambda \left \| w \right \|^2 + \sum_{n=1}^{N}{E(x^n,y^n,w)} + \sum_{n=1}^{N}{\log{Z}(x^n,w)}
\end{equation}
Having energy specified in the simplest form of linear dependence between weight vector and a feature vector, the only problematic term in this equation is a computation of a normalising constant Z as it would require summing over all possible configurations of labels for every training sample, and this is computationally impossible. Similarly as in the case of the inference problem, a Loopy Belief Propagation can be used to estimate the marginal probability and therefore a gradient needed for parameter update. However, this would mean that in each iteration for every training step the Loopy Belief Propagation has to be performed, which require a lot of time and resources. Instead, the second type of algorithms that have already been introduced in section \textit{\ref{sec:inference}: \nameref{sec:inference}}, namely Monte Carlo methods, can be applied. A stochastic variant of the Gradient Descent algorithm is used when the number of training examples is too high to compute a gradient efficiently \cite{optimisation_curtis}. The key idea is to update the parameters of the objective function not after processing a whole training set but after a random batch of training examples. In this way, the gradient is not computed exactly but only estimated. Tough this estimate is usually very rough and more iterations are required to converge, the simplification introduced by Stochastic Gradient Descent largely reduce the time of each iteration making the whole algorithm much faster. With such setting, the objective function can be presented as follows
\begin{equation}
    \pazocal{L} = \lambda \left \| w \right \|^2 + E(x^n,y^n,w) + \log{Z}(x^n,w)
\end{equation}
However, it is still computationally infeasible to find a minimum of such a function as the partition function still has to be computed. With the task of semantic image segmentation, it would require generating every possible configuration of pixel labels that is possible, which even for small images, with only a few classes available, requires a lot of computations. However, those calculations can also be simplified. Large populations usually have a lot of redundancy as they tend to contain very similar objects. Hence, instead of taking into account the whole population it is enough to propose such a set of samples which mean will reflect the probability distribution of the population. This concept is based on the notion of Monte Carlo integration \cite{bayesian_statistics}, which states that for sufficiently large number of independent samples $N$ summation of outputs for each sample is roughly equal to the integral of the function, as in equation below, with $p(x)$ denoting distribution of chosen samples, which may be uniform, but not necessarily.
\begin{equation}
    \int_{a}^{b}{f(x)p(x)dx} \cong \frac{1}{N}\sum_{i=1}^{N}{f(x_i)}
\end{equation}
This is of high use in computation of gradients, as instead of computing partition function, which is a summation of energy values for each possible configuration of labels it is enough to compute it for a much smaller number of samples. The main issue of this idea is that it is not known what is the required number of samples for the distribution to reflect the whole population. However, there are techniques named Markov Chain Monte Carlo methods to generate such samples that will overcome this problem \cite{markov_blake}. One of the most popular algorithms used for this task is named Gibbs Sampling, which creating samples that are dependent on each other. It is based on constructing a Markov Chain based on a conditional distribution of random variables. A Markov Chain is a model representing a sequence of states, through which it is possible to move freely with a probability depending only on one, previous state. Gibbs Sampling is an iterative algorithm which initially creates a random Markov Chain representing a configuration of labels for a given object. Then, in each iteration, every variable is replaced by the most probable state, given the conditional probability on all the other variables. Hence, for every possible label of a given variable, an energy of the whole configuration is calculated and a label with the lowest energy, meaning the highest probability of occurrence, is chosen to be involved in the final sequence. After every variable is processed, the resulting Markov Chain forms one sample, which is then used to initialise variables in a sequence for the next sample.  Then the objective function can be expressed as in equation \ref{eq:gibbs_sampling}, where $S$ denotes a set of Gibbs samples.
\begin{equation}
    \label{eq:gibbs_sampling}
    \pazocal{L} = \lambda \left \| w \right \|^2 + E(x^n,y^n,w) + \log{\frac{1}{S}\sum_{s \in S}{\exp{(-E(x^s,y^s,w))}}}
\end{equation}
The process of computing sampling distribution is far less demanding than the computation of full conditional distribution, as instead of taking into consideration every possible configuration of labels it is enough to process distribution of only one random variable at a time. However, if a chosen set of samples S is large, in every iteration still a lot of calculations are needed to be performed. Tough in this step yet another simplification can be introduced. For a set of samples containing only one element the number of computations is largely reduced. As an effect, again an estimate of a gradient will be very rough. However, as each Gibbs sample is created based on the previous sample with every iteration it is more close to the approximation of the true distribution. Hence, given a large number of iterations, the algorithm will eventually reach a direction which is an approximation of the true gradient. The key idea is not to choose the shortest path towards the minimum of the objective function, being a good approximation of the gradient, but instead to make more steps that are less accurate. Both approaches are aimed to converge near the global minimum, though the second one is faster as it is less computationally demanding. 

Hence, the final objective function that is to be used for parameter learning in the stochastic gradient descent algorithm involves regularisation term, energy component for a given training object n and energy of one Gibbs Sample $s$.
\begin{equation}
    \pazocal{L} = \lambda \left \| w \right \|^2 + E(x^n,y^n,w) -E(x^s,y^s,w)
\end{equation}
With such definition of the objective function it is possible to efficiently calculate a gradient and perform parameter learning. 
