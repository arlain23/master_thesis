Parameter estimation, also called parameter training, is a process that adjusts weights of the model so that they reflect real dependencies between input and outputs. In Conditional Random Fields properly estimated weights are necessary to compute factor energy during an inference process. In order to train parameters of the model supervised learning is used, which is a machine learning task that uses a ground truth to optimise an objective function so that it will be able to correctly map input into outputs. For the task of semantic image segmentation, ground truth is a set training samples which is composed of properly labelled images. Having them it is possible to construct such a factor graph in which not only features for observed variable nodes $X$ are known, but also labels for hidden nodes $Y$. Similarly to the task of inference, there are two main ways of parameter learning, first involving computation of marginal distributions named Maximum Likelihood Learning and the second requiring only MAP labelling \cite{markov_blake}. The latter method known as Max-Margin Learning has recently gained recognition as an alternative towards Maximum Likelihood Learning, however, it is based on Support Vector Machines and not on Conditional Random Fields, which are the key topic of this dissertation and therefore will not be described in this dissertation. 

\newpage
Maximum Likelihood Learning is a standard and the most popular approach to parameter learning in Conditional Random Fields. In order to perform any training process to find optimal parameters of the model, an objective function is required. This is a function that evaluates the ability of the model to properly map inputs into outputs given a chosen set of parameters. In Maximum Likelihood Learning the objective function is called likelihood and it is used to maximise conditional probability $p(y|x,w)$ based on a set of training objects denoted as $N$ with known inputs $x$ and outputs $y$, and an unknown weight vector $w$. Usually, the concept of log-likelihood is used instead of likelihood, as computations in logarithmic scale tend to be less complex. Furthermore, knowing that the state with the lowest energy of a factor graph is, in fact, the most probable state, the task of maximising probability is equivalent to minimising the energy of the system. Hence, the objective function for Maximum Likelihood Learning is expressed as a negative log-likelihood denoted as $\pazocal{L}$. Hence, basing on the definition of conditional probability $p(y|x,w)$ presented in equation \ref{eq:conditional_probability_energy} that can be transformed into logarithmic scale, the objective function $\pazocal{L}$ needed for the process of supervised learning can be expressed as in equation \ref{eq:regularisation_prob}, where $x^n$ and $y^n$ are inputs and outputs of the $n^{th}$ training sample.
\begin{equation}
     \label{eq:regularisation_prob}
    \pazocal{L}(w) = \lambda \left \| w \right \|^2 + \sum_{n=1}^{N}\bigg({E(x^n,y^n,w)} + {\log{Z}(x^n,w)}\bigg)
\end{equation}

The energy of a single sample can be expressed as a linear relation between a current weight vector $w$ and a feature function $\varphi$ that is dependent on inputs and outputs of the factor graph constructed for this sample, as presented in formula \ref{eq:energy_linear}. Then, the partition function $Z$ can be expressed as in equation \ref{eq:energy_linear_z}.
\begin{equation}
    \label{eq:energy_linear}
    E(x^n,y^n,w) = \left \langle \varphi(x^n,y^n),w \right \rangle 
\end{equation}
\begin{equation}
    \label{eq:energy_linear_z}
    Z = \sum_{y \in \pazocal{Y}}{\exp \Big(-\left \langle \varphi(x^n,y^n),w \right \rangle \Big)}
\end{equation}

Apart from an energy term calculated for each of $N$ training samples and their normalising constants $Z$, in the objective function there is also a regularisation term $\lambda$. Regularisation is one of the methods used to prevent overfitting of the model. Its aim is to penalise large values of weights so that less complex models are learned. As $\lambda$ is a hyperparameter of an objective function, it should be additionally chosen using for example cross-validation. Then, the process of parameter learning is aimed to find such a weight vector $w^*$ for which the objective function will be minimal, as in equation \ref{eq:regularisation}.
\begin{equation}
    \label{eq:regularisation}
    w^* = \argmin_{w}{\bigg(\lambda \left \| w \right \|^2 + \sum_{n=1}^{N}{E(x^n,y^n,w)} + \sum_{n=1}^{N}{\log{Z}(x^n,w)}\bigg)}
\end{equation}

Finding a minimum of a function is a typical task in machine learning and can be solved with various methods. The fundamental and most straightforward method of solving optimisation problems is named Gradient Descent \cite{optimisation_deep_learning}. It is an algorithm that updates parameters of an objective function in such a way to minimise an error between expected outputs of training examples and the actual output of the trained function. It is done by iteratively moving into a direction specified by a negative of a gradient of the objective. The algorithm starts by assigning arbitrary values to the parameters of the function and changing them slightly in each iteration, according to the calculated gradient. At each iteration $t+1$ parameters are updated by subtracting part of a value of the gradient. The size of this part is defined by a hyperparameter $\alpha$ named step size, which can be either constant or changeable throughout a process of learning. Weight update is done according to the formula \ref{eq:weight_update}.
\begin{equation}
    \label{eq:weight_update}
    w_{(t+1)} = w(t) - \alpha_t \nabla \pazocal{L}(w_t)
\end{equation}

The process of weight updating continues until the error is small enough to state that a local minimum, or global in case of convex functions, was reached. To apply the Gradient Descent method in an optimisation problem, an objective function has to be differentiable. Fortunately, a regularised conditional probability specified that was presented in formula \ref{eq:regularisation_prob}, is a smooth and convex function, hence Gradient Descent method can be used to find a global minimum of it.

During the training process, in each iteration value of gradient $\nabla \pazocal{L}(w)$ needs to be computed in order to update weights of the model. This requires summation over all training samples, which for large training sets is highly time-consuming. Furthermore, to obtain the part of the gradient that is associated with a partition function also a summation over all possible configurations of labels for every pixel is needed, which even for small images with only a few classes requires a lot of calculations. Formula for gradient calculation is presented in \ref{eq:gradient_init}.
\begin{equation}
    \label{eq:gradient_init}
    \nabla_w\pazocal{L}(w) = 2\lambda w + 
    \sum_{n=1}^{N}{ \bigg( \varphi(x^n,y^n) -
    \log{\sum_{y_z \in \pazocal{Y}}{\exp \Big( \varphi(x^n,y_{z}^{n}) \Big) }} \bigg)}
\end{equation}

Fortunately, it is not necessary to directly compute the gradient, as there are methods with which it is possible to approximate it. Similarly as in the case of the inference problem, a Loopy Belief Propagation can be used to estimate the marginal probability and therefore the gradient needed for parameter update. However, this would mean that in each iteration the Loopy Belief Propagation has to be performed, which require a lot of time and resources. Instead, the second type of algorithms that have already been introduced in \textit{section \ref{sec:inference}: \nameref{sec:inference}}, namely Monte Carlo methods, can be applied. One of such methods is a stochastic variant of the Gradient Descent algorithm which is used when the number of training examples is too high to compute the gradient efficiently \cite{optimisation_curtis}. The key idea of Stochastic Gradient Descent is to update the parameters of the objective function not after processing the whole training set but after a random batch of training examples, or only after just one example. In this way, the gradient is not computed exactly but only estimated. Tough this estimation is usually very rough and more iterations are required to converge, the simplification introduced by this method largely reduce the time of each iteration making the whole algorithm much faster. However, even if the sum over all training examples is not required, still in order to obtain the partition function the summation over all configurations of labels is needed, which is computationally infeasible. Fortunately, similarly as in the case of the gradient, the partition function does not need to be computed directly. Instead, another estimation is proposed, which is based on an assumption that large populations usually have a lot of redundancy as they tend to contain very similar objects. Hence, instead of taking into account the whole population it is enough to propose such a set of samples which will reflect the probability distribution of the population. This concept is based on the notion of Monte Carlo integration \cite{bayesian_statistics}, which states that for sufficiently large number of independent samples $S$ summation of outputs for each sample is roughly equal to the integral of the function, as in equation \ref{eq:monte_carlo_integral}, with $p(x)$ denoting distribution of chosen samples.
\begin{equation}
    \label{eq:monte_carlo_integral}
    \int_{a}^{b}{f(x)p(x)dx} \cong \frac{1}{S}\sum_{s=1}^{S}{f(x_s)}
\end{equation}
The main issue of this idea is that it is not known what is the required number of samples for the distribution to reflect the whole population. However, there are techniques named Markov Chain Monte Carlo methods to generate such samples that will fulfil the Monte Carlo integration principle. One of the most popular examples of these methods is named Gibbs Sampling. It is an algorithm that is used to create mutually dependent samples from the probability distribution of an underlying model. It works by constructing a Markov Chain based on a conditional distribution of random variables. A Markov Chain is a model representing a sequence of states, through which it is possible to move freely with a probability depending only on one, previous state. Gibbs Sampling is an iterative algorithm which initially creates a random Markov Chain representing a configuration of labels for a given object. Then, in each iteration, every variable is replaced by the most probable state, given the conditional probability on all the other variables. Hence, for every possible label of a given variable, an energy of the whole configuration is calculated and a label with the lowest energy, meaning the highest probability of occurrence, is chosen to be involved in the final sequence. After every variable is processed, the resulting Markov Chain forms one sample, which is then used to initialise variables in a sequence for the next sample. This forms a set of Gibbs samples created in such a way that they can be used to estimate the part of the gradient associated with the partition function $Z$. Thus, the formula for gradient estimation during a Stochastic Gradient Descent after the process of Gibbs Sampling may be expressed as in \ref{eq:gradient_samples}.
\begin{equation}
    \label{eq:gradient_samples}
    \nabla_w\pazocal{L}(w) = 2\lambda w + 
    \varphi(x^n,y^n) -
    \log{ \bigg(
        \frac{1}{S} \sum_{s = 1}^{S}{\exp \Big( \varphi(x^s,y^{s}) \Big) } \bigg)}
\end{equation}
The process of computing sampling distribution is far less demanding than the computation of full conditional distribution, as instead of taking into consideration every possible configuration of labels it is enough to process distribution of only one random variable at a time provided a set of samples that fulfil Monte Carlo integration principle. However, if a chosen set of samples $S$ is large, in every iteration still a lot of calculations are needed to be performed. Fortunately, in this step yet another simplification can be introduced. For a set of samples containing only one element the number of computations is largely reduced. As an effect, again an estimate of a gradient will be very rough, however, as each Gibbs sample is created based on the previous sample with every iteration it is more close to the approximation of the true distribution. Hence, given a large number of iterations, the algorithm will eventually reach a direction which is an approximation of the true gradient. This method of gradient estimation based on only one sample is known as contrastive divergence training \cite{Nowozin}. Thus, the final formula to compute the gradient needed for parameter update is presented in equation \ref{eq:gradient_final}. 
\begin{equation}
    \label{eq:gradient_final}
    \nabla_w\pazocal{L}(w) = 2\lambda w + \varphi(x^n,y^n) -\varphi(x^s,y^s)
\end{equation}


Typically Gradient Descent Methods tend to choose the shortest path towards the minimum of the objective function. The key idea behind all the presented simplifications is not to find the shortest path, but instead to make more steps that are less accurate but easier to compute. Both approaches are aimed to converge near the global minimum, though the second one is faster are more applicable for problems with a lot of training data and large output space. Having defined the objective function and a method to estimate the gradient it is possible to efficiently perform a supervised parameter learning and to define weights of the system that are required for the inference process.
