In machine learning very often there is a need to construct a mathematical model that represent real-life objects in terms of data essential to a given problem. An example of such representations is a graphical model, which may reflect not only relations between observations and quantities of interest but also between different observations. Such models offer a compact and intuitive way of representing a large set of related variables, which is of high use in Structured Prediction. 

Depending on an application, elements of a graph represent different objects, however, in the most basic form a graphical model can be presented as such a model that is composed of a set of nodes being a reflection of each random variable. If two or more variables are related, then their associated nodes are connected with an edge and with each edge, there may be a vector of parameters, or weights assigned that describe how much one variable is depended on the other. Also, a graph can model other information than relations between nodes. Giving an example, for image segmentation each node represents a pixel in an image, and edges describe relations between individual pixels. Apart from a layer of interconnected pixel nodes, there can be an additional layer created, with each node representing a class label. Then, between a pixel node and its label node, there is also an edge that describes in a clear way the relations between a pixel and its label.

The goal of constructing graphical models is to propose a data representation that will allow finding a solution for the specified problem in a clear and direct way. However, for some applications, especially for classification, it is impossible to propose a well-defined model that will give an answer with certainty. In such situations, it is beneficial to propose a representation of an object that encodes not direct, but statistical dependencies between observations and quantities of interest and probabilistic graphical models offer such functionality. By using them it is possible to obtain not a single, definite answer but a full probability distribution of answers, which is a mathematical function representing each possible result with a probability of its occurrence. 

Depending on a kind of a graph this probability distribution is modelled in a different way. There are two main types of graphical models, directed and undirected. Directed graphs, also known as Bayesian networks, have edges that have a direction from one node to the another, meaning that there is a one-way dependence between variables. Using those graphs causality between different variables can be modelled in a clear way. Bayesian networks encode conditional probability of every label of a node given that its parent nodes have a given label assigned. However, in many applications, especially in computer vision, variables do not have this kind of interactions. This is also a case in image processing, as there is no hierarchy between pixels in an image. Such problems can be modelled with undirected graphs named Markov Random Fields, which form a base of representing a problem domain in Structured Prediction. In this case,  instead of having conditional probability based on parent nodes, a joint probability distribution is modelled, which encodes the probability of every label of each variable occurring simultaneously. Unfortunately, for large input space, this involves a lot of data processing. In order to store information about it in an explicit way, for example in the form of a table, an enormous number of cells would be needed as every combination of all the variablesâ€™ possible values should be stored in a separate cell. Because of the fact that in Structured Prediction a quantity of variables is typically large, there is a need to propose different methods of encoding joint probability distributions that just a table and graphical models offer such functionality. In Markov Random Fields, all not connected variables are assumed to be conditionally independent, which means that variables depend only on their closest neighbours. Because of this, it is possible to decompose large distribution functions into smaller distributions containing only related variables, which significantly reduces the number of data needed to be stored and therefore the number of computations needed \cite{causality_pearl}. 
