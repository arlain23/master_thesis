In machine learning very often there is a need to construct a mathematical model that represent real-life objects in terms of data essential for a given problem. An example of such representations is a graphical model, which reflect not only relations between observations and quantities of interest but also between different observations. They offer a compact and intuitive way of representing a large set of related variables, which is of high use in structured prediction. 

A graphical model is composed of a set of nodes being a reflection of each random variable. Edges between nodes are associated with relations that occur between those variables. Furthermore, with each edge, there may be a vector of parameters, or weights assigned which describe how much one variable is depended on the other. Depending on an application, elements of a graph represent different objects, for example, each node can be a pixel in an image, and edges can describe relations between individual pixels. The goal of constructing graphical models is to propose a data representation that will allow finding a solution for the specified problem in a clear and direct way. However, for some applications, especially for classification, it is impossible to propose a well-defined model that will give an answer with certainty. In such situations, it is beneficial to propose a representation of an object that encodes not direct, but statistical dependencies between observations and quantities of interest. Probabilistic graphical models offer such functionality. By using them it is possible to obtain not a single, definite answer but a full probability distribution of answers, being a mathematical function representing each possible result with a probability of its occurrence. 

Depending on a kind of a graph this probability is modelled in a different way. There are two main graph types, directed and undirected graphical models. Directed graphs, also known as Bayesian networks, have edges that are directed from one node to the another, meaning that there is a one-way dependence between variables. Using those graphs causality between different variables can be modelled in a clear way. Bayesian networks encode conditional probability of every label of a node given that its parent nodes have a given label assigned.  However, in many applications, especially in computer vision, variables do not have this kind of interactions. This is also a case in image processing, as there is no hierarchy between pixels in an image. Such problems can be modelled with undirected graphs named Markov random fields, which form a base of representing a problem domain in structured prediction. In this case,  instead of modelling conditional probability based on parent nodes a joint probability distribution is modelled, which encodes the probability of every label of each variable occurring simultaneously. For large input space, this involves a lot of data processing. In order to store information about it in an explicit way, for example in the form of a table, an enormous number of cells would be needed as every combination of all the variablesâ€™ possible values should be stored in a separate cell. Because of the fact that in structured prediction a quantity of variables is typically large, there is a need to propose different methods of encoding joint probability distributions and graphical models offer such functionality. In Markov Random Fields, all not connected variables are assumed to be conditionally independent, which means that variables depend only on their closest neighbours. Because of this, it is possible to decompose large distribution functions into smaller distributions containing only related variables, which reduces the number of data needed to be stored \cite{causality_pearl}. 
