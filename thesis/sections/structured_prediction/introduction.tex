In various tasks of classification it is not enough to propose an output as a simple scalar value but instead, a complex structure or multiple related outputs like sequences, trees or graphs are needed. One of the examples of such problems is natural language processing, for instance, aimed to assign parts of speech for each word in a sentence \cite{markov_altun}. A different example would be a prediction of a structure of biological sequences like proteins or genes \cite{graph_liu}. Furthermore, such tasks are common in computer vision to detect and categorise objects present in an image or a video \cite{crf_torralba}. What is characteristic of all these kinds of applications is that very often the dimensionality of inputs is large. 

As a result, a number of potential outcomes can be enormous, making the classification computationally highly demanding. A framework that allows solving such problems is called structured prediction. It is a technique involving supervised machine learning algorithms in which the output is treated as one object instead of a set of individual values. Such an object has a complex structure containing a number of related entities, which can involve constraints or dependencies. Taking into account those relations is a key concept of structured prediction. 

In this chapter explanation of definitions and terms connected with structured prediction will be provided, which are necessary to understand this dissertation. First, the concept of probabilistic graphical models will be described, followed by an overview of the factorisation process. In the next section, Conditional Random Fields, which are a part of the topic of this dissertation, will be introduced. After that, a process of inference in factor graphs and parameter training will be described. Last, but not the least an overview of different methods of energy formulation will be provided. 