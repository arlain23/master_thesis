A process of decomposing a joint probability distribution is named factorisation. As a result of it, a set of factors is defined, which product is equal to a given joint probability distribution. Factorisation can largely decrease the complexity of calculations as it is only required to calculate the outcome of each factor within its scope being usually much smaller than the whole input domain. A base for defining a factor in undirected graphs is a clique that is a subset of interconnected nodes, in which each variable is connected to every other variable in a set. A clique which is not a subset of any other clique and therefore cannot be extended by adding any more nodes, is called a maximal clique \cite{clique}. In undirected graphical models, a joint probability $p(k)$ of every variable from a set $K$ is understood as a normalised product of factors $\psi_{c}$ of every clique $c$ in a set of all maximal cliques $C$. 
\begin{equation}
    p(k)=\frac{1}{Z}\prod_{c\in C}\psi_{c}({y_c})
\end{equation}
In those graphs, factors also called clique potentials, do not represent conditional probability. Instead, they are non-negative functions with a property that values with higher potentials are more probable to occur. Because of this, a family of joint probabilities does not need to sum up to 1. That is why a normalising constant $Z$ is introduced. It is called a partition function, which is a summation over joint probabilities of all variables $k$.
\begin{equation}
    Z=\sum_{k\in K}\prod_{c\in C}\psi_{c}({y_c}) 
\end{equation}
Factorisation can be presented on a graph which is fully connected, however, it is not very convenient. A far more intuitive way of modelling factorisation of a joint probability distribution is to use factor graphs, which are a type of undirected, probabilistic graphical models. Such graphs contain two kinds of nodes â€“ variable nodes, and factor nodes, with each factor node being connected to variable nodes that depend on it. Then to calculate joint probability it is enough to compute a product of each factor.
\begin{equation}
    p(k)=\frac{1}{Z}\prod_{f\in \pazocal{F}}\psi_{f}({y_f})
\end{equation}
\begin{equation}
     Z=\sum_{k\epsilon K}\prod_{f\in \pazocal{F}}\psi_{f}({y_f})
\end{equation}
For directed graphs it was clear that factors represent conditional probability, however, for undirected graphs, the meaning of factors is not defined. The only requirement for them is to be non-negative as probability cannot be negative. One of the easiest ways of ensuring that factors will output a positive value is to present a problem in an exponential domain, with use of energy function. Energy is such a function which exponential represents a factor potential. Then, a joint probability can be expressed as a summation of energies for each factor. 
\begin{equation}
    p(k)=\frac{1}{Z}\exp(-\sum_{f\in \pazocal{F}}{E_f}({y_f})) 
\end{equation}
\begin{equation}
    Z=\sum_{k\in K}\exp(-\sum_{f\in \pazocal{F}}{E_f}({y_f}))
\end{equation}
Similarly to a factor potential, energy \cite{factor_graphs_chopra} represents how accurate is a given solution. It is done by assigning a scalar value to each configuration of random variables with an interpretation that the lower an energy is, the more probable is a given configuration.  Then to obtain a variable k with the highest probability it is enough to specify a variable with the lowest energy.
\begin{equation}
    \argmax p(K=k) = \argmin \sum_{f\in \pazocal{F}} {E_f}({y_f})
\end{equation}
Hence, finding the most likely solution for a problem is interchangeable with identifying the state which energy of all factors is the lowest. 

This concept is useful for supervised learning in which there is a need to provide a set of examples mapping input values into outputs, which in case of structured prediction is in a complex, structural form \cite{crf_sutton}. Both inputs denoted as X and outputs Y can be understood as random variables that are to be predicted. With the use of factor graphs this problem can be modelled to obtain a joint probability distribution P(X,Y). Modelling the whole distribution of each possible output Y on each possible observation X can be difficult to obtain especially when a lot of observed features are taken into account, as they largely increase the number of possible combinations of X and Y.
