The methods of probability distribution were already explained in theory in \textit{chapter \ref{chapter:structured_prediction}: \nameref{chapter:structured_prediction}}. This estimation is conducted separately for every feature and it is based on a training set of images. For the first type of features, which are dependent of a colour of neighbouring superpixel and its position in a grid estimation is pretty simple. Because of the fact that each such feature is discrete it is enough to go through the whole training set and count how many times a given colour appears on a given position. Then, by dividing this number by the total superpixels in all training images the probability of a given feature is obtained. For every feature, probabilities of each labels are cached, and then in a training and inference process conditional probability $p(y_i|f_i)$ can be computed straight away.

For features representing a percentage of colours in the neighbourhood of superpixels from the already described grid, probability distribution is more difficult to obtain. Those features have continuous values and therefore it is not possible to count how many times a given occurs in the training set. Initially in the thesis this distribution was computed using Parzen-Rosenblatt Kernel estimation. This allowed to estimate a probability of an unknown sample for which the distribution was modelled. However, this method had one, huge drawback as in order to obtain this estimation, it is necessary to iterate through every training sample. During the training process, in each epoch and for every superpixel in every image there was a need to compute this probability. Furthermore, while constructing a Gibbs sample the procedure had to be repeated for every label in every position in a created Markov chain. Because of this, the resources needed to for the training process were enormous, even for small datasets. Therefore, another method of probability distribution estimation had to be implemented. Similarly, as in case of colour-position features it was necessary to choose such a method that will model this distribution only once, and give a probability of an unknown sample without a need of excessive computation. This can be obtained by means of probability histograms, in which the whole distribution is divided into a limited set of bins, and each bin is characterised by its probability of occurrence. The number of bins is a next hyperparameter of the described system. Tough the colour percentage features have continuous values, they are quite repetitive as the number of neighbouring superpixels is roughly the same and therefore, the number of histogram bins does not need to be large. 

Having constructed histograms for each feature and each label, a conditional probability of a label on a given feature can be obtained. A description of histograms are used to calculate this probability will be given on an example of features number 73,74 and 75. These features represent percentage of red, green and blue neighbours of a middle superpixel, being a superpixel for which the label is to be predicted. Figure \ref{fig:nonlinear_histogram_red} shows histograms for \nth{73} feature, divided into 17 bins constructed based o a grid $7 \times 7$ elements and the neighbourhood size of two. The upper border value of each bin is marked on the x axis of the presented histogram. 
\begin{figure}[!htb]
    \centering
    \includegraphics[width=\textwidth]{nonlinear_intro/histograms/red.png}
    \caption{Histograms for \nth{73} feature - percentage of red neighbours.}
    \label{fig:nonlinear_histogram_red}
\end{figure}
From this figure, the dependence between colours of neighbouring pixels and a label is clearly visible. If all neighbouring pixels are red then the most probable is label for a given superpixel is 0. If not all, but the majority of superpixels are red, then either 0 or 3 label are winning. On the other hand, if no neighbouring superpixels are red, than there is no chance of this superpixel to be classified with label 0, nor 3. The next histogram, for \nth{75} feature, is presented in figure \ref{fig:nonlinear_histogram_green}.
\begin{figure}[!htb]
    \centering
    \includegraphics[width=\textwidth]{nonlinear_intro/histograms/green.png}
    \caption{Histograms for \nth{74} feature - percentage of green neighbours}
    \label{fig:nonlinear_histogram_green}
\end{figure}
This histogram shows how green neighbours affect the label of the superpixel under consideration. If there is around 80\% or more of green neighbours than the highest probability of occurrence has the label 2. For smaller number of green neighbours, label 3 is the most probable, however, if there are no green superpixels in the neighbourhood then the given superpixel is not a part of the object of class 3. Last histogram, presented in figure \ref{fig:nonlinear_histogram_blue} was created for \nth{75} feature.
\begin{figure}[!htb]
    \centering
    \includegraphics[width=\textwidth]{nonlinear_intro/histograms/blue.png}
    \caption{Histograms for \nth{74} feature - percentage of blue neighbours}
    \label{fig:nonlinear_histogram_blue}
\end{figure}
From this histogram it can be read, that label 3 is assigned to the given superpixel only if there are no blue superpixels in the neighbourhood, which is true because letter h is always surrounded by a green region. The more blue the neighbourhood is, the more probable is the label 2.

With the use of such histograms probability distribution for every feature and label was modelled. In order to get a conditional probability of labels for an unknown sample represented by one feature, it is enough to check in which bin a value of this feature falls and read its probability from the respective histogram. By analysing such histograms for a combination of different features it is possible for Conditional Random Fields to learn a shape of objects that are present in an image. The resulting conditional probability is an output of the feature function for unary potential that was chosen for this part of the system.